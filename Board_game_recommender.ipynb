{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Authors:\n",
    "Bj√∂rn Lindgren,\n",
    "Fredrik Askeroth\n",
    "\n",
    "Source code used in the thesis: \"Enhancing Board Game Recommendations: Leveraging K-nearest neighbors in Collaborative Filtering\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from scipy.sparse import csr_matrix\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ratings = pd.read_csv(\"dataset/user_ratings.csv\")\n",
    "df_games = pd.read_csv(\"dataset/games.csv\")\n",
    "df = pd.merge(df_ratings, df_games[['BGGId', 'Name']], \n",
    "              on='BGGId', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_counts = df.isnull().sum()\n",
    "print(null_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check if any rows are duplicates\n",
    "duplicates = df.duplicated().value_counts()\n",
    "print(duplicates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove duplicate ratings for the same game by the same user (due to different version of the game etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the mean ratings back to the original DataFrame based on 'Username' and 'Name'\n",
    "# Filter the DataFrame to keep only the original rows (non-duplicates) and the calculated mean ratings\n",
    "# Drop the original 'Rating' column and rename the mean rating column\n",
    "# Reindex the columns to move 'Rating' to the second position\n",
    "mean_ratings = df.groupby(['Username', 'BGGId'])['Rating'].mean()\n",
    "df_removed_duplicates = df.merge(mean_ratings, \n",
    "            on=['Username', 'BGGId'], suffixes=('', '_mean'))\n",
    "df_unique = df_removed_duplicates[~df_removed_duplicates.\n",
    "            duplicated(subset=['Username', 'BGGId'], keep=False)]\n",
    "df_dropped = df_unique.drop(columns=['Rating']).rename(columns=\n",
    "            {'Rating_mean': 'Rating'})\n",
    "column_order = ['BGGId', 'Rating', 'Username', 'Name']\n",
    "df_dropped = df_dropped.reindex(columns=column_order)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate average rating for users and add average rating and adjusted rating column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Group by 'Username' and calculate the average rating for each user\n",
    "# Step 2: Merge the average ratings back into the original DataFrame\n",
    "# Step 3: Subtract the average rating from each rating to get the adjusted rating\n",
    "# Now df contains the adjusted ratings in the 'Adjusted_Rating' column\n",
    "user_avg_ratings = df_dropped.groupby(\n",
    "    'Username')['Rating'].mean().reset_index()\n",
    "df_adjusted_mean = pd.merge(df_dropped, user_avg_ratings, \n",
    "                            on='Username', suffixes=('', '_avg'))\n",
    "df_adjusted_mean['Adjusted_Rating'] = df_adjusted_mean[\n",
    "    'Rating'] - df_adjusted_mean['Rating_avg']\n",
    "column_order = ['BGGId', 'Rating', 'Rating_avg', \n",
    "                'Adjusted_Rating', 'Username', 'Name']\n",
    "df_adjusted_mean = df_adjusted_mean.reindex(columns=column_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_adjusted_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by 'Username' and count the number of rows for each group\n",
    "grouped_user_counts = df_adjusted_mean.groupby('Username').size()\n",
    "grouped_user_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by 'BGGId' and count the number of rows for each group\n",
    "grouped_counts = df_adjusted_mean.groupby('BGGId').size()\n",
    "grouped_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_ratings = 3000\n",
    "rating_counter = df_adjusted_mean['BGGId'].value_counts()\n",
    "filtered_out = rating_counter[rating_counter < game_ratings].index\n",
    "games_mask = df_adjusted_mean['BGGId'].isin(filtered_out)\n",
    "df_adjusted_mean.drop(index=df_adjusted_mean[games_mask].index, \n",
    "                      inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_adjusted_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_ratings = 100\n",
    "rating_counter = df_adjusted_mean['Username'].value_counts()\n",
    "filtered_out = rating_counter[rating_counter < user_ratings].index\n",
    "user_mask = df_adjusted_mean['Username'].isin(filtered_out)\n",
    "df_adjusted_mean.drop(index=df_adjusted_mean[user_mask].index, \n",
    "                      inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_adjusted_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by 'Username' and count the number of rows for each group\n",
    "grouped_user_counts = df_adjusted_mean.groupby('Username').size()\n",
    "grouped_user_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by 'BGGId' and count the number of rows for each group\n",
    "grouped_counts = df_adjusted_mean.groupby('BGGId').size()\n",
    "grouped_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_by_username = df_adjusted_mean.groupby('Username').size().sort_values(ascending=False)\n",
    "grouped_by_username"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_adjusted_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_ratings = dict(zip(df_adjusted_mean['Username'], df_adjusted_mean['Rating_avg']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the dataset into training and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_split(data, random_state, test_size=0.2):    \n",
    "    train_to_concat = []\n",
    "    test_to_concat = []\n",
    "    for user, group in data.groupby('Username'):\n",
    "        train_user, test_user = train_test_split(group, test_size=test_size, train_size=1-test_size, random_state=random_state)\n",
    "        train_to_concat.append(train_user)\n",
    "        test_to_concat.append(test_user)\n",
    "    \n",
    "    train = pd.concat(train_to_concat, ignore_index=True)\n",
    "    test = pd.concat(test_to_concat, ignore_index=True)\n",
    "\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have used two specific random states when splitting the dataset for reproduceability\n",
    "train_set, test_set = user_split(data=df_adjusted_mean, \n",
    "                                 random_state=0, test_size=0.2)\n",
    "# train_set, test_set = user_split(data=df_adjusted_mean, \n",
    "#                                 random_state=1, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create datastructures for efficient computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used to calculate MAE\n",
    "actual_ratings_test_set = {}\n",
    "for _, row in test_set.iterrows():\n",
    "    username = row['Username']\n",
    "    bggid = row['BGGId']\n",
    "    rating = row['Rating']\n",
    "    \n",
    "    if username not in actual_ratings_test_set:\n",
    "        actual_ratings_test_set[username] = {}\n",
    "    \n",
    "    actual_ratings_test_set[username][bggid] = rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used to calculate predicted rating\n",
    "actual_ratings_train_set = {}\n",
    "for _, row in train_set.iterrows():\n",
    "    username = row['Username']\n",
    "    bggid = row['BGGId']\n",
    "    rating = row['Rating']\n",
    "    \n",
    "    if username not in actual_ratings_train_set:\n",
    "        actual_ratings_train_set[username] = {}\n",
    "    \n",
    "    actual_ratings_train_set[username][bggid] = rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used to calculate MAE\n",
    "rated_items_test_set = {}\n",
    "for index, row in test_set.iterrows():\n",
    "    username = row['Username']\n",
    "    bggid = row['BGGId']\n",
    "    \n",
    "    if username not in rated_items_test_set:\n",
    "        rated_items_test_set[username] = []\n",
    "    \n",
    "    rated_items_test_set[username].append(bggid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used to calculate predicted rating\n",
    "rated_items_train_set = {}\n",
    "for index, row in train_set.iterrows():\n",
    "    username = row['Username']\n",
    "    bggid = row['BGGId']\n",
    "    \n",
    "    if username not in rated_items_train_set:\n",
    "        rated_items_train_set[username] = []\n",
    "    \n",
    "    rated_items_train_set[username].append(bggid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used in precision and recall calculations\n",
    "relevant_items_df = test_set.loc[test_set['Rating'] >= 7]\n",
    "relevant_items = {}\n",
    "\n",
    "for username, bggid in relevant_items_df.groupby('Username')['BGGId']:\n",
    "    bggid_list = list(bggid)\n",
    "    relevant_items[username] = bggid_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used to recommend items for evaluation\n",
    "items_test_set = {}\n",
    "for username, bggid in test_set.groupby('Username')['BGGId']:\n",
    "    bggid_list = list(bggid)\n",
    "    items_test_set[username] = bggid_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot the DataFrame so a user matrix, so rows represent users and columns represent items\n",
    "user_item_matrix = train_set.pivot(index='Username', \n",
    "                                   columns='BGGId', \n",
    "                                   values='Rating').fillna(0)\n",
    "user_item_matrix_with_nan = train_set.pivot(index='Username', \n",
    "                                            columns='BGGId', \n",
    "                                            values='Adjusted_Rating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used to recommend items\n",
    "not_rated_items_train_set = {}\n",
    "for username, row in user_item_matrix_with_nan.iterrows():\n",
    "    nan_bggid_list = []\n",
    "    for bggid, value in row.items():\n",
    "        if pd.isna(value):\n",
    "            nan_bggid_list.append(bggid)\n",
    "    not_rated_items_train_set[username] = nan_bggid_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transpose the user-item matrix to obtain an item-user matrix\n",
    "item_user_matrix = user_item_matrix.T\n",
    "item_user_matrix_adjusted = user_item_matrix_with_nan.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_adjusted_cosine_similarity(matrix_transposed):\n",
    "    sparse_matrix = csr_matrix(matrix_transposed.values)\n",
    "    num_items = sparse_matrix.shape[0]\n",
    "\n",
    "    similarity_matrix = np.zeros((num_items, num_items))\n",
    "\n",
    "    for i in range(num_items):\n",
    "        for j in range(i, num_items):  # Only compute upper triangle (similarity_matrix is symmetric)\n",
    "            row_i = sparse_matrix.getrow(i)\n",
    "            row_j = sparse_matrix.getrow(j)\n",
    "\n",
    "            # Find common indices and adjust to exclude columns with NaN values\n",
    "            common_indices = np.where(~np.isnan(row_i.toarray()) & ~np.isnan(row_j.toarray()))[1]\n",
    "    \n",
    "            if common_indices.size > 0:\n",
    "                non_missing_values_i = row_i[:, common_indices].toarray().flatten()\n",
    "                non_missing_values_j = row_j[:, common_indices].toarray().flatten()\n",
    "\n",
    "                if len(non_missing_values_i) > 0 and len(non_missing_values_j) > 0:\n",
    "                    similarity = cosine_similarity([non_missing_values_i], [non_missing_values_j])[0, 0]\n",
    "                    similarity_matrix[i, j] = similarity\n",
    "                    similarity_matrix[j, i] = similarity\n",
    "\n",
    "    return similarity_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate item similarity\n",
    "# Create a DataFrame with item names as both rows and columns\n",
    "# Set similarity to -5 for each item to itself, so that the similarity for each item to itself is less than the similarity to all other items\n",
    "# Convert similarity to distances (inversely proportional to similarities)\n",
    "item_similarity = calculate_adjusted_cosine_similarity(\n",
    "    item_user_matrix_adjusted)\n",
    "item_similarity_df = pd.DataFrame(\n",
    "    item_similarity, \n",
    "    index=item_user_matrix_adjusted.index, \n",
    "    columns=item_user_matrix_adjusted.index)\n",
    "np.fill_diagonal(item_similarity_df.values, -5)\n",
    "distance_matrix = 1 - item_similarity_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_nearest_neighbors(distance_matrix, BGGId, nearest_neighbors_model):\n",
    "    distances, neighbor_indices = nearest_neighbors_model.kneighbors([distance_matrix[BGGId]], return_distance=True)\n",
    "\n",
    "    return distances, neighbor_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_rated_neighbors(neighbor_distances, neighbor_indices, rated_items):\n",
    "    neighbor_indices = neighbor_indices[0]\n",
    "    neighbor_distances = neighbor_distances[0]\n",
    "    neighbors = item_user_matrix.iloc[neighbor_indices].index.tolist()\n",
    "    rated_neighbors = []\n",
    "    rated_neighbors_distances = []\n",
    "\n",
    "    for i in range(len(neighbors)):\n",
    "        if neighbors[i] in rated_items:\n",
    "            rated_neighbors.append(neighbors[i])\n",
    "            rated_neighbors_distances.append(neighbor_distances[i])\n",
    "    \n",
    "    return rated_neighbors_distances, rated_neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predicted_rating(item, username, distance_matrix, nearest_neighbors_model):\n",
    "    rated_items = rated_items_train_set[username]\n",
    "    distances, neighbor_indices = find_nearest_neighbors(distance_matrix, item, nearest_neighbors_model)\n",
    "    rated_neighbors_distances, rated_neighbors = find_rated_neighbors(distances, neighbor_indices, rated_items)\n",
    "\n",
    "    if len(rated_neighbors) != 0:\n",
    "        numerator = 0\n",
    "        denominator = 0\n",
    "        for i in range(len(rated_neighbors)):\n",
    "            neighbor = rated_neighbors[i]\n",
    "            weight = 1 - rated_neighbors_distances[i] #transform distance into similarity\n",
    "            user_rating = actual_ratings_train_set[username][neighbor]\n",
    "            numerator += (weight * user_rating)\n",
    "            denominator += weight\n",
    "        predicted_rating_item = numerator / denominator\n",
    "        return predicted_rating_item\n",
    "    else:\n",
    "        average_rating = average_ratings[username]\n",
    "        return average_rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Main recommendation function\n",
    "def recommend_items(distance_matrix, username, top_N, \n",
    "                    nearest_neighbors_model):\n",
    "    not_rated_items = not_rated_items_train_set[username]\n",
    "    predicted_ratings = []\n",
    "    predicted_items = []\n",
    "\n",
    "    for item in not_rated_items:\n",
    "        predicted_rating_item = get_predicted_rating(item, username, distance_matrix, nearest_neighbors_model)\n",
    "        predicted_ratings.append(predicted_rating_item)\n",
    "        predicted_items.append(item)  \n",
    "    \n",
    "    recommended_items = pd.DataFrame(predicted_ratings, index=predicted_items, columns=['Predicted rating'])\n",
    "    recommended_items.index.name = 'BGGId'\n",
    "    sorted_recommendations = recommended_items.sort_values(by=['Predicted rating'], ascending=False)\n",
    "    top_N_recommendations = sorted_recommendations.head(top_N)\n",
    "    top_N_list = top_N_recommendations.index.tolist()\n",
    "    return top_N_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adjusted recommendation function for evaluation\n",
    "def recommend_items_test_set(distance_matrix, username, top_N, nearest_neighbors_model):\n",
    "    items = items_test_set[username]\n",
    "    predicted_ratings = []\n",
    "    predicted_items = []\n",
    "\n",
    "    for item in items:\n",
    "        predicted_rating_item = get_predicted_rating(item, username, distance_matrix, nearest_neighbors_model)\n",
    "        predicted_ratings.append(predicted_rating_item)\n",
    "        predicted_items.append(item)  \n",
    "    \n",
    "    recommended_items = pd.DataFrame(predicted_ratings, index=predicted_items, columns=['Predicted rating'])\n",
    "    recommended_items.index.name = 'BGGId'\n",
    "    sorted_recommendations = recommended_items.sort_values(by=['Predicted rating'], ascending=False)\n",
    "    top_N_recommendations = sorted_recommendations.head(top_N)\n",
    "    top_N_list = top_N_recommendations.index.tolist()\n",
    "    return top_N_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_precision(recommended_items, relevant_items):\n",
    "    true_positives = 0\n",
    "    for item in recommended_items:\n",
    "        if item in relevant_items:\n",
    "            true_positives += 1\n",
    "    \n",
    "    return true_positives / len(recommended_items)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_precision(recommended_items, relevant_items):\n",
    "    if not recommended_items or not relevant_items:\n",
    "        return 0.0\n",
    "\n",
    "    num_hits = 0\n",
    "    sum_precisions = 0\n",
    "    for i, item in enumerate(recommended_items):\n",
    "        if item in relevant_items:\n",
    "            num_hits += 1\n",
    "            precision_at_i = num_hits / (i + 1)\n",
    "            sum_precisions += precision_at_i\n",
    "\n",
    "    if num_hits == 0:\n",
    "        return 0.0\n",
    "\n",
    "    return sum_precisions / num_hits    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_recall(recommended_items, relevant_items):\n",
    "    if not recommended_items or not relevant_items:\n",
    "        return 0.0\n",
    "\n",
    "    num_hits = 0\n",
    "    sum_recalls = 0\n",
    "    total_relevant_items = len(relevant_items)\n",
    "    \n",
    "    if total_relevant_items == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    for i, item in enumerate(recommended_items):\n",
    "        if item in relevant_items:\n",
    "            num_hits += 1\n",
    "            recall_at_i = num_hits / total_relevant_items\n",
    "            sum_recalls += recall_at_i\n",
    "    \n",
    "    if num_hits == 0:\n",
    "        return 0.0\n",
    "\n",
    "    return sum_recalls / num_hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_map_calculation(users, nearest_neighbors_model, top_N):\n",
    "    results_average_precision = []\n",
    "\n",
    "    for user in users:\n",
    "        recommended_items = recommend_items(distance_matrix, user, top_N, nearest_neighbors_model)\n",
    "        relevant_items_user = relevant_items[user]\n",
    "        ap = average_precision(recommended_items, relevant_items_user)\n",
    "        results_average_precision.append(ap)\n",
    "\n",
    "    return np.sum(results_average_precision) / len(users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcaulates MAP and MAR on recommendations based on all items\n",
    "def run_map_mar_calculation(users, nearest_neighbors_model, top_N):\n",
    "    results_average_precision = []\n",
    "    results_average_recall = []\n",
    "\n",
    "    for user in users:\n",
    "        recommended_items = recommend_items(distance_matrix, user, \n",
    "                                    top_N, nearest_neighbors_model)\n",
    "        relevant_items_user = relevant_items[user]\n",
    "        ap = average_precision(recommended_items, \n",
    "                               relevant_items_user)\n",
    "        results_average_precision.append(ap)\n",
    "        ar = average_recall(recommended_items, relevant_items_user)\n",
    "        results_average_recall.append(ar)\n",
    "\n",
    "\n",
    "    return np.sum(results_average_precision) / len(users), np.sum(\n",
    "        results_average_recall) / len(users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates MAP and MAR on recommendations based on items with known preferences\n",
    "def run_map_mar_items_test_set(users, nearest_neighbors_model, top_N):\n",
    "    results_average_precision = []\n",
    "    results_average_recall = []\n",
    "\n",
    "    for user in users:\n",
    "        recommended_items = recommend_items_test_set(distance_matrix, user, top_N, nearest_neighbors_model)\n",
    "        relevant_items_user = relevant_items[user]\n",
    "        ap = average_precision(recommended_items, relevant_items_user)\n",
    "        results_average_precision.append(ap)\n",
    "        ar = average_recall(recommended_items, relevant_items_user)\n",
    "        results_average_recall.append(ar)\n",
    "\n",
    "\n",
    "    return np.sum(results_average_precision) / len(users), np.sum(results_average_recall) / len(users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_recall(recommended_items, relevant_items):\n",
    "    true_positives = 0\n",
    "    for item in recommended_items:\n",
    "        if item in relevant_items:\n",
    "            true_positives += 1\n",
    "    \n",
    "    return true_positives / len(relevant_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcaulates precision and recall on recommendations based on all items\n",
    "def run_precision_recall_calculation(users, nearest_neighbors_model, top_N):\n",
    "    results_precision = []\n",
    "    results_recall = []\n",
    "\n",
    "    for user in users:\n",
    "        recommended_items = recommend_items(distance_matrix, user, top_N, nearest_neighbors_model)\n",
    "        relevant_items_user = relevant_items[user]\n",
    "        precision = calculate_precision(recommended_items, relevant_items_user)\n",
    "        results_precision.append(precision)\n",
    "        recall = calculate_recall(recommended_items, relevant_items_user)\n",
    "        results_recall.append(recall)\n",
    "\n",
    "    return np.sum(results_precision) / len(users), np.sum(results_recall) / len(users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates precision and recall on recommendations based on items with known preferences\n",
    "def run_precision_recall_items_test_set(users, nearest_neighbors_model, top_N):\n",
    "    results_precision = []\n",
    "    results_recall = []\n",
    "\n",
    "    for user in users:\n",
    "        recommended_items = recommend_items_test_set(distance_matrix, user, top_N, nearest_neighbors_model)\n",
    "        relevant_items_user = relevant_items[user]\n",
    "        precision = calculate_precision(recommended_items, relevant_items_user)\n",
    "        results_precision.append(precision)\n",
    "        recall = calculate_recall(recommended_items, relevant_items_user)\n",
    "        results_recall.append(recall)\n",
    "\n",
    "    return np.sum(results_precision) / len(users), np.sum(results_recall) / len(users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mean_absolute_error(username, distance_matrix, nearest_neighbors_model):\n",
    "    actual_ratings = []\n",
    "    predicted_ratings = []\n",
    "    rated_items_testset = rated_items_test_set[username]\n",
    "\n",
    "    for item in rated_items_testset:\n",
    "        predicted_ratings.append(get_predicted_rating(item, username, distance_matrix, nearest_neighbors_model))\n",
    "        actual_rating = actual_ratings_test_set[username][item]\n",
    "        actual_ratings.append(actual_rating)\n",
    "    \n",
    "    mae = mean_absolute_error(actual_ratings, predicted_ratings)\n",
    "    return mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_sample(random_seed, size):\n",
    "    sample = set()\n",
    "    num_users, num_items = user_item_matrix.shape\n",
    "    np.random.seed(random_seed)\n",
    "    while len(sample) < size:\n",
    "        random_user_index = np.random.randint(0, num_users)\n",
    "        random_user = user_item_matrix.index[random_user_index]\n",
    "        sample.add(random_user)\n",
    "    return sample\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_mean_absolute_error_calculation(users, \n",
    "                                        nearest_neighbors_model):\n",
    "    results = []\n",
    "    for user in users:\n",
    "        mae = calculate_mean_absolute_error(\n",
    "            user, distance_matrix, nearest_neighbors_model)\n",
    "        results.append(mae)\n",
    "    return np.sum(results) / len(users)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used in all MAE experiments\n",
    "# Experiments are performed using random_seed 0 and 1 for two different split of the dataset into training and test set\n",
    "user_sample = get_user_sample(1, 1000)\n",
    "# user_sample = get_user_sample(0, 1000)\n",
    "k_values = [5,10,15,20,25,30,35,40,45,50,100]\n",
    "results_mae_user_sample_1 = {}\n",
    "for k in k_values:\n",
    "    nearest_neighbors_model = NearestNeighbors(n_neighbors=k, metric='precomputed')\n",
    "    nearest_neighbors_model.fit(distance_matrix.values)\n",
    "    result = run_mean_absolute_error_calculation(\n",
    "        user_sample, nearest_neighbors_model)\n",
    "    print(\"Mean absolute error with k = \",k ,\"and num_users =\", \n",
    "          len(user_sample),\":\", result)\n",
    "    results_mae_user_sample_1[k] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used in MAP and MAR experiment 1 and 2\n",
    "# Calculated MAP and MAR on recommendations based on all items\n",
    "# Experiments are performed using random_seed 0 and 1\n",
    "random_seed = 0\n",
    "#random_seed = 1\n",
    "user_sample = get_user_sample(random_seed, 1000)\n",
    "k_values = [5,10, 20, 30] \n",
    "topN = 10\n",
    "results_map_user_sample_0 = {}\n",
    "results_mar_user_sample_0 = {}\n",
    "for k in k_values:\n",
    "    nearest_neighbors_model = NearestNeighbors(n_neighbors=k, \n",
    "                                          metric='precomputed')\n",
    "    nearest_neighbors_model.fit(distance_matrix.values)\n",
    "    result_map, result_mar = run_map_mar_calculation(user_sample, \n",
    "                                    nearest_neighbors_model, topN)\n",
    "    print(\"Mean Average Precision with k = \",k , \"and topN =\", \n",
    "          topN, \"and num_users =\", len(user_sample),\":\", result_map)\n",
    "    results_map_user_sample_0[k] = result_map\n",
    "    print(\"Mean Average Recall with k = \",k ,\"and num_users =\", \n",
    "          len(user_sample),\":\", result_mar)\n",
    "    results_mar_user_sample_0[k] = result_mar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used in MAP and MAR experiment 3 and 4\n",
    "# Calculated MAP and MAR on recommendations based on items with known preferences\n",
    "# Experiments are performed using random_seed 0 and 1\n",
    "random_seed = 0\n",
    "#random_seed = 1\n",
    "user_sample = get_user_sample(random_seed, 1000)\n",
    "k_values = [5,10, 20, 30] \n",
    "topN = 10\n",
    "results_map_user_sample_01 = {}\n",
    "results_mar_user_sample_01 = {}\n",
    "for k in k_values:\n",
    "    nearest_neighbors_model = NearestNeighbors(n_neighbors=k, metric='precomputed')\n",
    "    nearest_neighbors_model.fit(distance_matrix.values)\n",
    "    result_map, result_mar = run_map_mar_items_test_set(user_sample, nearest_neighbors_model, topN)\n",
    "    print(\"Mean Average Precision with k = \",k , \"and topN =\", topN, \"and num_users =\", \n",
    "          len(user_sample),\":\", result_map)\n",
    "    results_map_user_sample_01[k] = result_map\n",
    "    print(\"Mean Average Recall with k = \",k ,\"and num_users =\", \n",
    "          len(user_sample),\":\", result_mar)\n",
    "    results_mar_user_sample_01[k] = result_mar"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
